### A Hierarchical Recurrent Encoder-Decoder For Generative Context-Aware Query Suggestion

This code is the pytorch implementation of the following paper:

"<a href="https://arxiv.org/pdf/1507.02221.pdf">A Hierarchical Recurrent Encoder-Decoder For Generative Context-Aware Query Suggestion</a>", by Alessandro Sordoni, Yoshua Bengio, Hossein Vahabi, Christina Lioma, Jakob G. Simonsen, Jian-Yun Nie, appeared in CIKM'15.

Official implementation of the paper is available at https://github.com/sordonia/hred-qs.

<p align="justify">
Proposed hierarchical recurrent encoder-decoder architecture is shown below.
<p align="justify">

<p align="center">
<br>
<img src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/00f31994c123fbceba5d5a0e06bb78aff651aae7/3-Figure3-1.png" width="90%">
<br>
<p align="center">

<p align="justify">
Figure: The hierarchical recurrent encoder-decoder (HRED). The user types cleveland gallery -> lake erie art. During training, the model encodes cleveland gallery, updates the session-level 
recurrent state and maximize the probability of seeing the following query lake erie art. The process is repeated for all queries in the session. During testing, a contextual suggestion is generated by 
encoding the previous queries, by updating the session-level recurrent states accordingly and by sampling a new query from the last obtained session-level recurrent state. In the example, the generated 
contextual suggestion is cleveland indian art.
<p align="justify">